# DRL_train.py
import torch
import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import DummyVecEnv
from PyQt6.QtCore import QObject, pyqtSignal

from comsol_surrogate_train import SurrogateNN

# =========================================================
# 1ï¸âƒ£ åŠ è½½ surrogate æ¨¡å‹
# =========================================================
checkpoint_path = "surrogate_model_optimized.pth"
checkpoint = torch.load(checkpoint_path, map_location="cpu", weights_only=False)

input_dim = 13
output_dim = 6

model_surrogate = SurrogateNN(input_dim, output_dim)
model_surrogate.load_state_dict(checkpoint["model_state_dict"])
model_surrogate.eval()

x_mean = checkpoint["x_scaler_mean"]
x_scale = checkpoint["x_scaler_scale"]
y_mean = checkpoint["y_scalers_mean"]
y_scale = checkpoint["y_scalers_scale"]

# =========================================================
# 2ï¸âƒ£ å®šä¹‰å¥–åŠ±ç±»å’Œç¯å¢ƒç±»
# =========================================================
class RewardCalculator:
    def __init__(self, w=None, smooth_alpha=0.9):
        self.smooth_alpha = smooth_alpha
        self.prev_reward = None
        self.w = w or {"w1": 1.0, "w2": 1.0, "w3": 1.0, "w4": 1.0, "w5": 1.0, "w6": 1.0}
        self.reward_max = sum(self.w.values())

    def compute(self, L_error, k_error, Ripple_ratio, Volume_ratio, Loss_ratio, Temp_ratio,
                L_target=1.0, k_target=1.0):
        L_err_norm = np.clip(L_error / (L_target + 1e-6), 0, 1)
        k_err_norm = np.clip(k_error / (k_target + 1e-6), 0, 1)
        Ripple_norm = np.clip(Ripple_ratio, 0, 1)
        Vol_norm = np.clip(Volume_ratio, 0, 1)
        Loss_norm = np.clip(Loss_ratio, 0, 1)
        Temp_norm = np.clip(Temp_ratio, 0, 1)

        reward = (
            self.w["w1"] * (1 - L_err_norm)
            + self.w["w2"] * (1 - k_err_norm)
            + self.w["w3"] * (1 - Ripple_norm)
            + self.w["w4"] * (1 - Vol_norm)
            + self.w["w5"] * (1 - Loss_norm)
            + self.w["w6"] * (1 - Temp_norm)
        )
        reward = np.clip(reward / self.reward_max, 0.0, 1.0)

        if self.prev_reward is None:
            smooth_reward = reward
        else:
            smooth_reward = self.smooth_alpha * self.prev_reward + (1 - self.smooth_alpha) * reward

        self.prev_reward = smooth_reward
        return smooth_reward


class SurrogateEnv(gym.Env):
    metadata = {"render_modes": ["human"], "render_fps": 30}

    def __init__(self, mode="highfreq", target_L=10.0, target_k=0.95, custom_weights=None):
        super().__init__()
        self.input_dim = 13
        self.output_dim = 6

        # ç©ºé—´å®šä¹‰
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.input_dim,), dtype=np.float32)
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.input_dim,), dtype=np.float32)

        self.state = np.zeros(self.input_dim, dtype=np.float32)
        self.target_L = target_L
        self.target_k = target_k
        self.mode = mode

        self.step_count = 0
        self.max_steps_per_episode = 50

        # å¥–åŠ±å‡½æ•°æƒé‡
        default_weights = {
            "highfreq": dict(w1=1, w2=1, w3=0.2, w4=0.2, w5=0.25, w6=0.15),
            "highpower": dict(w1=1, w2=1, w3=0.2, w4=0.15, w5=0.2, w6=0.25)
        }
        self.w = custom_weights if custom_weights is not None else default_weights[self.mode]

        # ===== åˆ›å»º RewardCalculator =====
        self.reward_calc = RewardCalculator(w=self.w, smooth_alpha=0.9)

        # å‚è€ƒå€¼
        self.ref = dict(LCoil=2, Lmut=1.5, Ripple=3, Volume=10.19, Loss=18000, Temp=72)

        # ===== è‡ªåŠ¨åŠ è½½æ•°æ®è®¡ç®—ä¸Šä¸‹é™ =====
        input_cols = ["base_x", "base_z", "base_y", "g_1", "g_2", "g_3",
                      "thick_copper", "w_1", "w_2", "core_y", "r", "n", "I"]
        data = pd.read_csv("comsol_data.csv")[input_cols]

        self.input_bounds = {}
        for col in input_cols:
            mean, std = data[col].mean(), data[col].std()
            lower, upper = mean - 3 * std, mean + 3 * std
            # ä¿è¯éè´Ÿ
            lower = max(0.0, lower)
            self.input_bounds[col] = (float(lower), float(upper))

        self.lower_bounds = np.array([v[0] for v in self.input_bounds.values()], dtype=np.float32)
        self.upper_bounds = np.array([v[1] for v in self.input_bounds.values()], dtype=np.float32)

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        # ä»å®é™…ç‰©ç†èŒƒå›´éšæœºåˆå§‹åŒ–
        self.state = np.array([np.random.uniform(l, h) for l, h in zip(self.lower_bounds, self.upper_bounds)],
                              dtype=np.float32)
        self.step_count = 0
        return self.state, {}

    def step(self, action):
        # é™åˆ¶åŠ¨ä½œèŒƒå›´åœ¨çœŸå®åŸŸå†…
        self.state = np.clip(self.state + action, self.lower_bounds, self.upper_bounds)
        self.state = np.maximum(self.state, 0.0)  # éè´Ÿçº¦æŸ

        # ===== ç¦»æ•£å˜é‡å¤„ç† =====
        # å‡è®¾ thick_copper å¯¹åº” state[1]ï¼Œn å¯¹åº” state[2]
        thick_copper_idx = 6
        n_idx = 11

        allowed_thick = np.array([0.035, 0.07, 0.105])
        allowed_n = np.arange(1, 9)  # [1,2,3,4,5,6,7,8]

        # å°†åŠ¨ä½œå€¼æ˜ å°„åˆ°æœ€è¿‘çš„ç¦»æ•£å€¼
        def discretize_value(raw_val, lower, upper, allowed_values):
            mapped_idx = int(np.clip(np.round((raw_val - lower) / (upper - lower) * (len(allowed_values) - 1)),
                                     0, len(allowed_values) - 1))
            return allowed_values[mapped_idx]

        self.state[thick_copper_idx] = discretize_value(
            self.state[thick_copper_idx],
            self.lower_bounds[thick_copper_idx],
            self.upper_bounds[thick_copper_idx],
            allowed_thick
        )

        self.state[n_idx] = discretize_value(
            self.state[n_idx],
            self.lower_bounds[n_idx],
            self.upper_bounds[n_idx],
            allowed_n
        )

        # ===== surrogate é¢„æµ‹ =====
        x_scaled = (self.state - x_mean) / x_scale
        x_tensor = torch.tensor(x_scaled, dtype=torch.float32).unsqueeze(0)
        with torch.no_grad():
            y_scaled = model_surrogate(x_tensor).cpu().numpy()[0]
        y_real = np.array([y_scaled[i] * y_scale[i] + y_mean[i] for i in range(self.output_dim)])

        LCoil, Lmut, Ripple, Volume, Loss, Temp = y_real
        k = Lmut / LCoil if LCoil != 0 else 0.0

        # ===== è¯¯å·®é¡¹ =====
        L_error = abs(LCoil - self.target_L) / (abs(self.target_L) + 1e-12)
        k_error = abs(k - self.target_k) / (abs(self.target_k) + 1e-12)

        # ===== æ¯”å€¼ =====
        Ripple_ratio = float(Ripple / self.ref["Ripple"])
        Volume_ratio = float(Volume / self.ref["Volume"])
        Loss_ratio = float(Loss / self.ref["Loss"])
        Temp_ratio = float(Temp / self.ref["Temp"])

        # ===== å¥–åŠ±å‡½æ•° =====
        '''
        reward = (
            self.w["w1"] * np.exp(-L_error ** 2)
            + self.w["w2"] * np.exp(-k_error ** 2)
            - self.w["w3"] * np.tanh(Ripple_ratio)
            - self.w["w4"] * np.tanh(Volume_ratio)
            - self.w["w5"] * np.tanh(Loss_ratio)
            - self.w["w6"] * np.tanh(Temp_ratio)
        )
        '''


        reward = self.reward_calc.compute(
            L_error=L_error,
            k_error=k_error,
            Ripple_ratio=Ripple_ratio,
            Volume_ratio=Volume_ratio,
            Loss_ratio=Loss_ratio,
            Temp_ratio=Temp_ratio,
            L_target=self.target_L,
            k_target=self.target_k
        )


        # ===== è¾¹ç•Œæƒ©ç½š =====
        boundary_penalty = np.mean(
            (self.state <= self.lower_bounds + 1e-6) | (self.state >= self.upper_bounds - 1e-6)
        )
        reward -= 0.5 * boundary_penalty

        reward = float(reward)
        self.step_count += 1
        terminated = bool(self.step_count >= self.max_steps_per_episode)
        truncated = False

        info = {
            "pred": y_real,
            "k": float(k),
            "reward": reward,
            "LCoil": float(LCoil),
            "Lmut": float(Lmut),
            "thick_copper": float(self.state[thick_copper_idx]),
            "n": int(self.state[n_idx])
        }

        return self.state, reward, terminated, truncated, info





# =========================================================
# 3ï¸âƒ£ å°è£…ä¸º PyQt ä¿¡å·ç±»
# =========================================================
class TrainingWorkerBackend(QObject):
    log_msg = pyqtSignal(str)
    epoch_result = pyqtSignal(int, float, float)  # epoch, loss, avg_reward
    progress = pyqtSignal(int)
    finished_signal = pyqtSignal()
    top3_signal = pyqtSignal(list)  # ç”¨äºå‘å‡ºå‰ä¸‰å‚æ•°

    def __init__(self, config: dict):
        super().__init__()
        self.config = config

    def run_training(self):
        # é…ç½®éƒ¨åˆ†
        mode_map = {"é«˜é¢‘æ¨¡å¼": "highfreq", "å¤§åŠŸç‡æ¨¡å¼": "highpower"}
        mode = mode_map.get(self.config.get("scene", "highfreq"), "highfreq")
        target_L = self.config.get("L_target", 2.0)
        target_k = self.config.get("M_target", 0.5)

        if mode == "highfreq":
            custom_weights = {"w1": 1, "w2": 1, "w3": 0.2, "w4": 0.2, "w5": 0.25, "w6": 0.15}
        elif mode == "highpower":
            custom_weights = {"w1": 1, "w2": 1, "w3": 0.2, "w4": 0.15, "w5": 0.2, "w6": 0.25}

        env = SurrogateEnv(mode=mode, target_L=target_L, target_k=target_k, custom_weights=custom_weights)

        env = DummyVecEnv([lambda: env])

        total_epochs = int(self.config.get("epochs", 50))
        ppo_model = PPO(
            "MlpPolicy",
            env,
            verbose=0,
            learning_rate=self.config.get("lr", 0.001),
            batch_size=self.config.get("batch_size", 64),
        )
        self.log_msg.emit("ğŸš€ å¼€å§‹è®­ç»ƒ PPO + Surrogate...")

        for epoch in range(1, total_epochs + 1):
            # æ¯ä¸€ä»£è®­ç»ƒ
            ppo_model.learn(total_timesteps=1000)

            # è®°å½•æ¯ä¸€ä»£æŒ‡æ ‡
            L_errs, k_errs, Ripple_ratios, Volume_ratios, Loss_ratios, Temp_ratios = [], [], [], [], [], []
            LCoils, Lmuts = [], []

            obs = env.reset()
            step_rewards = []  # âœ… å­˜å‚¨æ¯æ­¥å¥–åŠ±

            for step_i in range(10):
                action, _ = ppo_model.predict(obs, deterministic=True)
                obs, reward, done, info = env.step(action)
                step_rewards.append(float(reward))

                # âœ… æ‰“å°æ¯æ­¥å¥–åŠ±åˆ°æ—¥å¿—
                self.log_msg.emit(f"    Step {step_i + 1} reward = {float(reward):.6f}")

                # æå– info æ•°æ®
                info_dict = info[0] if isinstance(info, list) else info
                y_pred = info_dict.get("pred", [0] * 6)
                k_val = info_dict.get("k", 0)

                # LCoil å’Œ Lmut
                LCoil = y_pred[0]
                Lmut = y_pred[1]

                # è®¡ç®—å„é¡¹è¯¯å·®ä¸æ¯”å€¼
                L_error = abs(LCoil - target_L) / (abs(target_L) + 1e-12)
                k_error = abs(k_val - target_k) / (abs(target_k) + 1e-12)
                Ripple_ratio = y_pred[2] / env.envs[0].ref["Ripple"]
                Volume_ratio = y_pred[3] / env.envs[0].ref["Volume"]
                Loss_ratio = y_pred[4] / env.envs[0].ref["Loss"]
                Temp_ratio = y_pred[5] / env.envs[0].ref["Temp"]

                # è®°å½•æŒ‡æ ‡
                L_errs.append(L_error)
                k_errs.append(k_error)
                Ripple_ratios.append(Ripple_ratio)
                Volume_ratios.append(Volume_ratio)
                Loss_ratios.append(Loss_ratio)
                Temp_ratios.append(Temp_ratio)
                LCoils.append(LCoil)
                Lmuts.append(Lmut)

            # âœ… æ‰“å°æ¯è½®å¥–åŠ±ç»Ÿè®¡
            sum_reward = float(np.sum(step_rewards))
            avg_reward = float(np.mean(step_rewards))
            self.log_msg.emit(f"    Step rewards sum = {sum_reward:.6f}, avg = {avg_reward:.6f}")

            # è®¡ç®—å¹³å‡å€¼
            avg_L = float(np.mean(L_errs))
            avg_k = float(np.mean(k_errs))
            avg_Ripple = float(np.mean(Ripple_ratios))
            avg_Volume = float(np.mean(Volume_ratios))
            avg_Loss = float(np.mean(Loss_ratios))
            avg_Temp = float(np.mean(Temp_ratios))
            avg_LCoil = float(np.mean(LCoils))
            avg_Lmut = float(np.mean(Lmuts))

            # è¾“å‡ºæ—¥å¿—
            self.log_msg.emit(
                f"ğŸ” Epoch {epoch} avg metrics -> "
                f"L_err={avg_L:.3f}, k_err={avg_k:.3f}, "
                f"Ripple={avg_Ripple:.3f}, Vol={avg_Volume:.3f}, "
                f"Loss={avg_Loss:.3f}, Temp={avg_Temp:.3f}, "
                f"LCoil={avg_LCoil:.3f}, Lmut={avg_Lmut:.3f}"
            )

            self.epoch_result.emit(epoch, 0, avg_reward)
            self.progress.emit(int(epoch / total_epochs * 100))
            self.log_msg.emit(f"Epoch {epoch}/{total_epochs} - Avg Reward: {avg_reward:.4f}")

        # ä¿å­˜æ¨¡å‹
        ppo_model.save("ppo_surrogate_model")

        # ğŸŒŸ åœ¨è®­ç»ƒç»“æŸåæ‰§è¡Œç­–ç•¥è¯„ä¼°ï¼Œä¼ å…¥è®­ç»ƒæ—¶ä½¿ç”¨çš„ mode
        self.log_msg.emit("âœ… PPO æ¨¡å‹å·²ä¿å­˜ï¼Œå¼€å§‹ç­–ç•¥è¯„ä¼°")
        top3_results = self.evaluate_policy_top3(mode=mode)

        # å°†å‰ä¸‰ç»„å‚æ•°å†™å…¥æ—¥å¿—å¹¶å‘ä¿¡å·
        for i, (reward, params) in enumerate(top3_results, 1):
            # âœ… æ ¼å¼åŒ–å‚æ•°è¾“å‡ºä¸ºä¸‰ä½å°æ•°
            formatted_params = "\n  ".join(
                [f"{k}: {v:.3f}" if isinstance(v, (float, int)) else f"{k}: {v}" for k, v in params.items()]
            )
            msg = f"ğŸ† Top-{i} | Reward={reward:.3f}\n  {formatted_params}"
            self.log_msg.emit(msg)

        # å‘å‡ºä¿¡å·ç»™å‰ç«¯é¡µé¢
        self.top3_results = top3_results
        self.top3_signal.emit(top3_results)

        self.finished_signal.emit()



    # =========================================================
    # è®­ç»ƒå®Œæˆåçš„ç­–ç•¥è¯„ä¼°å‡½æ•°ï¼ˆå–å¥–åŠ±æœ€é«˜çš„å‰ä¸‰ç»„å‚æ•°ï¼‰
    # =========================================================
    def evaluate_policy_top3(self, model_path="ppo_surrogate_model.zip", eval_episodes=100, mode="highfreq"):
        """
        è¯„ä¼° PPO ç­–ç•¥ï¼Œè¿”å›å¥–åŠ±æœ€é«˜çš„å‰ä¸‰ç»„å‚æ•°ã€‚
        mode: "highfreq" æˆ– "highpower"ï¼Œä¿è¯æƒé‡ä¸è®­ç»ƒä¸€è‡´
        """
        # æ ¹æ®æ¨¡å¼é€‰æ‹©æƒé‡
        if mode == "highfreq":
            custom_weights = {"w1": 1, "w2": 1, "w3": 0.2, "w4": 0.2, "w5": 0.25, "w6": 0.15}
        elif mode == "highpower":
            custom_weights = {"w1": 1, "w2": 1, "w3": 0.2, "w4": 0.15, "w5": 0.2, "w6": 0.25}
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å¼ {mode}ï¼Œè¯·é€‰æ‹© 'highfreq' æˆ– 'highpower'")

        # åˆ›å»ºç¯å¢ƒ
        env = SurrogateEnv(mode=mode, custom_weights=custom_weights)
        env = DummyVecEnv([lambda: env])

        # åŠ è½½ PPO æ¨¡å‹
        model = PPO.load(model_path)

        results = []  # å­˜å‚¨ (reward, params_dict)

        for ep in range(eval_episodes):
            reset_result = env.reset()
            if isinstance(reset_result, tuple):
                obs = reset_result[0]
            else:
                obs = reset_result

            for _ in range(env.envs[0].max_steps_per_episode):
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, info = env.step(action)
                if done:
                    break

            # æœ€ç»ˆå¥–åŠ±
            final_reward = float(reward)

            # ä» info æå–ç¦»æ•£åçš„ thick_copper å’Œ n
            info_dict = info[0] if isinstance(info, list) else info
            state = env.envs[0].state

            params = {
                "base_x": state[0],
                "base_z": state[1],
                "base_y": state[2],
                "g_1": state[3],
                "g_2": state[4],
                "g_3": state[5],
                "thick_copper": info_dict.get("thick_copper", float(state[6])),
                "w_1": state[7],
                "w_2": state[8],
                "core_y": state[9],
                "r": state[10],
                "n": info_dict.get("n", int(round(state[11]))),
                "extra": state[12],
            }

            results.append((final_reward, params))

        # æŒ‰å¥–åŠ±æ’åºå–å‰ä¸‰
        top3 = sorted(results, key=lambda x: x[0], reverse=True)[:3]
        return top3
